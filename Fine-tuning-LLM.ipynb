{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from datetime import datetime\n",
    "\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# check your torch version make sure it is GPU version\n",
    "\n",
    "from torch.cuda import is_available\n",
    "print(is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "\n",
    "# set up accelerator may not be necessary for QLoRA\n",
    "#device = 'cuda:0'\n",
    "#device = torch.device(device)\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc76e91e0bb4642b904b5d6737dd74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/407 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\datasets--ymoslem--Law-StackExchange. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drink and riding (bicycle) offence in Germany\n",
      "<p>I am charged with a criminal offence in a drink-and-riding-a-bicycle case. I have received a letter mentioning the alcohol content 1.66 promile. I have accepted the charges and agreed to pay the fine.</p>\n",
      "<ol>\n",
      "<li>What are the possible fines for this offence?</li>\n",
      "<li>Is there any possible legal effect on my resident permit?</li>\n",
      "<li>How long will the record stay in the register?</li>\n",
      "</ol>\n",
      "<p>I do not have a driving license.</p>\n",
      "\n",
      "[{'answer_id': 55185, 'body': '<ol>\\n<li><p>The fine would typically be around your monthly income.<br />\\n<sub>Legal basis: drunk driving per §316 StGB is punishable by up to one year in prison, but per §47 and §40 StGB short sentences are converted to a fine that depends on your daily net income (Tagessätze).</sub></p>\\n</li>\\n<li><p>There is likely no impact. Despite this being a crime, it will not appear in your criminal record that some employers need.<br />\\n<sub>Legal basis: Per §32 BZRG the criminal record will not show convictions with ≤ 90 Tagessätze. However, other government departments can request full access.</sub></p>\\n</li>\\n<li><p>You will get three “<a href=\"https://en.wikipedia.org/wiki/Point_system_(driving)#Germany\" rel=\"nofollow noreferrer\">Flensburg points</a>” which has no immediate effect, but can lead to increased penalties for your <em>next</em> traffic violation. Where the points have been awarded for a crime, they will expire after 5 or 10 years. You can voluntarily visit seminars to remove points at a rate of one per five years.<br />\\n<sub>Legal basis: per Appendix 13 FeV, drunk driving (§316 StGB) gives you two or three Flensburg points, depending on whether your license (implied for bikes) is suspended and the amount of alcohol in your bloodstream. Retention period is covered in §29 StVG. Seminars are covered in §4(7) StGB.</sub></p>\\n</li>\\n</ol>\\n<p>Other effects:</p>\\n<ul>\\n<li>You may be ordered to do the <a href=\"https://en.wikipedia.org/wiki/Medical-psychological_assessment_(Germany)\" rel=\"nofollow noreferrer\">MPU psychological evaluation</a> and can be banned from using a bike if you fail.</li>\\n<li>You will also have to take an MPU if you want to get a driving license.</li>\\n</ul>\\n', 'score': 4}, {'answer_id': 55183, 'body': '<p>Germany: Actually, you have a license to ride a bicycle on public roads. You get it automatically, you have no paper as evidence, <em>but it can be taken away</em>. You can lose the right to drive a bicycle in Germany, and 1.66‰ is <em>very</em> drunk. Don’t go anywhere on your bicycle if you intend to drink lots of alcohol.</p>\\n<p>1.6‰ on a bicycle with no irregular behaviour can get you a criminal charge, you are just above that. 0.3‰ <em>with</em> irregular behaviour can do the same. It’s a serious matter. Not serious enough to affect a resident permit probably, unless you repeat it.</p>\\n<p>Fines may be based on your income. If that is the case, lying about your income would be very serious, don’t do it.</p>\\n', 'score': 2}]\n"
     ]
    }
   ],
   "source": [
    "# load in datasets\n",
    "train_dataset = load_dataset('jonathanli/law-stack-exchange', split='train')\n",
    "\n",
    "train_dataset = train_dataset.shuffle(seed=1234)\n",
    "# print to see the data point\n",
    "print(train_dataset[134][\"body\"])\n",
    "print(train_dataset[134][\"text_label\"])\n",
    "print(train_dataset[134][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1239073e01f44b0d84c5b3d43353c6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load in the model\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# quantize to save memeory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "####### Tokenization  #######\n",
    "#############################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is a question that describes a problem about law. Write a response that could become the title of the problem.\"\n",
    "    system_message = \"Use the provided input to create a title that could have been used to generate the response with an LLM.\"\n",
    "    response = data_point[\"title\"].replace(original_system_message, \"\").replace(\"\\n\\n### Title\\n\", \"\").replace(\"\\n### Body\\n\", \"\").strip()\n",
    "    input = data_point[\"body\"]\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + input\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += eos_token\n",
    "    # full_prompt =f\"\"\"Given a question about punishments when breaking laws. Answer with the correct punishments.\n",
    "\n",
    "    #                 ### Question:\n",
    "    #                 {data_point[\"question_body\"]}\n",
    "\n",
    "    #                 ### Answer:\n",
    "    #                 {data_point[\"answers\"]}\n",
    "    #                \"\"\"\n",
    "    return tokenize(full_prompt)\n",
    "####################################\n",
    "####### End of Tokenization  #######\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 27332, 3133, 3112, 28747, 13, 8543, 272, 3857, 2787, 298, 2231, 264, 3941, 369, 829, 506, 750, 1307, 298, 8270, 272, 2899, 395, 396, 16704, 28755, 28723, 13, 13, 27332, 11232, 28747, 13, 28737, 837, 7101, 719, 288, 356, 264, 2572, 12181, 28733, 23158, 286, 21732, 2488, 298, 1950, 264, 5181, 28725, 395, 799, 9909, 28723, 415, 4099, 302, 272, 2696, 622, 347, 6431, 28723, 13, 13, 13, 28737, 837, 3653, 456, 4993, 395, 586, 25325, 28725, 298, 2231, 264, 5181, 354, 5088, 297, 272, 8932, 1834, 28723, 1136, 1259, 28725, 315, 506, 6140, 298, 272, 9909, 369, 272, 25325, 622, 927, 298, 1216, 272, 5181, 28725, 298, 6842, 20855, 304, 5657, 378, 582, 297, 3401, 28725, 304, 13965, 349, 16066, 1799, 297, 272, 12181, 4993, 28723, 13, 13, 13, 28737, 3091, 369, 1167, 2112, 302, 13945, 460, 14473, 2203, 1059, 3332, 28733, 28735, 1574, 21732, 26218, 28723, 2993, 28725, 315, 837, 521, 15950, 302, 272, 22570, 28723, 315, 3091, 369, 707, 1338, 28742, 28713, 771, 1580, 347, 5907, 304, 23253, 28725, 579, 315, 28742, 28715, 737, 298, 1460, 574, 7478, 356, 767, 2112, 302, 2877, 460, 1159, 528, 28723, 13, 13, 13, 24091, 272, 277, 5722, 302, 264, 3930, 659, 9934, 754, 272, 6202, 2696, 28725, 541, 369, 347, 14178, 1024, 272, 4099, 302, 272, 2488, 28804, 1047, 5081, 28725, 349, 736, 396, 4073, 1197, 3558, 28725, 442, 8622, 298, 913, 354, 456, 1871, 28804, 13, 13, 13, 1014, 799, 3551, 28725, 682, 347, 298, 2111, 272, 18128, 404, 264, 4098, 297, 272, 2496, 28723, 2993, 28725, 378, 993, 347, 369, 590, 460, 459, 6348, 297, 369, 28725, 304, 315, 682, 459, 737, 298, 1300, 3561, 438, 272, 948, 302, 272, 2488, 395, 264, 2696, 369, 315, 3573, 938, 28725, 442, 369, 3573, 6842, 15104, 325, 28737, 837, 835, 624, 302, 272, 18128, 404, 609, 851, 2996, 349, 298, 2380, 910, 298, 2318, 28725, 1671, 1250, 13957, 2598, 18925, 356, 272, 7335, 395, 586, 1951, 840, 11796, 28723, 4922, 1316, 682, 347, 1215, 22359, 28723, 13, 13, 13, 13, 13, 27332, 12107, 28747, 13, 19658, 1706, 9934, 297, 264, 15609, 2488, 354, 25325, 4099, 2, 2]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# print to see examples\n",
    "print(tokenized_train_dataset[4]['input_ids'])\n",
    "print(len(tokenized_train_dataset[4]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
     ]
    }
   ],
   "source": [
    "# setup LoRA\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.20.3\n",
      "Transformers version: 4.44.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import transformers\n",
    "\n",
    "print(\"Numpy version:\", numpy.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 1000 (from args) != 5000 (from trainer_state.json)\n",
      "\tsave_steps: 1000 (from args) != 5000 (from trainer_state.json)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\User\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\SideProject\\wandb\\run-20240913_091011-x7y2j2ly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wu7115-uci/huggingface/runs/x7y2j2ly' target=\"_blank\">mistral-law-stack-exchange-2024-09-13-09-09</a></strong> to <a href='https://wandb.ai/wu7115-uci/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wu7115-uci/huggingface' target=\"_blank\">https://wandb.ai/wu7115-uci/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wu7115-uci/huggingface/runs/x7y2j2ly' target=\"_blank\">https://wandb.ai/wu7115-uci/huggingface/runs/x7y2j2ly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88479f9624a54c9595206a0358786957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:477: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "project = \"law-stack-exchange\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=70000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=1000,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=1000,               # Save checkpoints every 5000 steps\n",
    "        # evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        # eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=False,                # Perform evaluation at the end of training\n",
    "        # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train('C:/Users/User\\Desktop/SideProject/content/drive/MyDrive/mistral-law-stack-exchange/checkpoint-5000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
